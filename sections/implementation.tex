\chapter{Implementation} \label{cha: implementation}
This Chapter presents the implementation of the SR and ReID together with the overall implementation choices.

\section{Dataloaders}
\label{imp_dataloading}
Both of the data loaders are built using the Dataset and DataLoader classes from torch.utils.data. They are designed to generate batches of data samples for training, validation and test. Special for the \ac{ReID} data loader is that validation and test is split into query and gallery splits. 
\\\\
For the \ac{ReID} datasets, three datasets are used for training: MSMT17, Market1501 and CUHK03. The data loader is combining these datasets, enabling joint training across all of them. To prevent any person-id conflict, in case where the same person-id is appearing in multiple dataset, and ensure continues id-values a json file with mappings is created. This maps from the old id, with a dataset specific prefix in front, to a new continues value. It ensures that all person IDs remain unique across dataset, while preserving person-id for each person and introducing continues values for which a softmax can be trained on. In the process of creating the json file, every 10th new person-id are pulled into a validation set. In the data loader, one sample per person ID is added to the query set. The remaining samples are categorized for the gallery set: any sample that does not match both the person-ID and camera-ID of the query sample is added to the gallery.
\\\\
The data loader for the SR datasets simply splits the images into input and target images, and creates batches. The images from DIV2K and RELLISUR dataset are kept separate.

\section{Dataaugmentation} \label{sec: dataaugmentation}
A classic way to improve performance and generalization is through augmentations. The goal is to improve model performance in terms of generalization across different image scales and resolution, and robustness in real life scenarios. The different types of augmentation and their purpose is included in Table \ref{tab:augmentationtypes}.
\begin{table}[H]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}p{0.18\linewidth}|>{\centering\arraybackslash}p{0.78\linewidth}|}\hline
         \cellcolor[HTML]{D8E9F7} \textbf{Augmentation}&  \cellcolor[HTML]{D8E9F7} \textbf{Purpose}\\\hline
         Normalization&  Standardize pixel intensity values to improve training.\\\hline
         Horizontal flipping&  Increase size bigger dataset.\\\hline
 Gaussian blur&Improve robustness to low-quality and blured images.\\\hline
 Gaussian noise&Enhance generalization to imperfect and noisy images.\\\hline
 Brightness change&Increase resilience to varying lighting conditions.\\\hline
 Resizing&Introduces scale variation by randomly reducing the image size with a scale between 0.5 and 1 and then padding back to the target dimensions. Resizing using bilinear interpolation.\\\hline
 Padding& Secure that tensor sizes is the same in order to be stacked.\\\hline
 JPEG compression& Improve robustness to compression noise, simulating compression from surveillance footage.\\ \hline
    \end{tabular}
    \caption{The different augmentations implemented.}
    \label{tab:augmentationtypes}
\end{table}
\noindent These data augmentations are used in three different transformation pipelines: One with light augmentations, one with strong augmentations, and one for evaluations, see Table \ref{tab:three_subtables}
\begin{table}[H]
\centering
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Light Training Augmentation}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Normalization
    \item Horizontal flipping
    \item Padding, if too small
    \item Resizing, if too big
    \vspace{6.1em}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentations used for baseline version of OSNet.}
\end{subtable}
\hfill
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Strong Training Augmentation}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Gaussian noise
    \item Gaussian Blur
    \item Brightness change
    \item Resizing
    \item JPEG compression
    \item Normalization
    \item Horizontal flipping
    \vspace{1em}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentation used for augmentation version of OSNet.}
\end{subtable}
\hfill
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Evaluation Preprocessing}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Normalization
    \vspace{11em}
    \vspace{1mm}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentation used for validation and test sets.}
\end{subtable}

\caption{The three different augmentations pipelines.}
\label{tab:three_subtables}
\end{table}
\noindent The three augmentations pipelines will be used in Chapter \ref{cha: experiments}.

\section{Implementation of OSNet}
%Note: vi bruger x1 modellen, som er den største. 
The architecture of the person \acs{ReID} model that is implemented is the same as described in Section \ref{lab:methodreid}. The \acf{OSNet} implementation is taken from the \texttt{Torchreid} library, which comes with a pretrained \acs{OSNet} model. However, to ensure full control over the training process and to train from scratch, a custom training script is made. 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}\hline
        \cellcolor[HTML]{D8E9F7} \textbf{Parameter} & \cellcolor[HTML]{D8E9F7} \textbf{Value} \\\hline
        No. Classes (\texttt{num\_classes}) & \texttt{num\_classes} \\\hline
        Block types (\texttt{blocks})             & [\texttt{OSBlock}, \texttt{OSBlock}, \texttt{OSBlock}] \\\hline
        Layers per stage (\texttt{layers})        & [2, 2, 2] \\\hline
        Channels per stage (\texttt{channels})    & [64, 256, 384, 512] \\\hline
        Loss function (\texttt{loss})             & \texttt{loss parameter ('softmax' or 'triplet')} \\\hline
    \end{tabular}
        \caption{OSNet\_x1\_0 Configuration.}
    \label{tab:osnet-config}
\end{table}

\subsubsection{Model Configuration}
The \acs{OSNet} x1 variant is used, which is the largest base \acs{OSNet} model with 2.2 million parameters. As shown in Table \ref{tab:osnet-config} the model consists of three stages of OSBlocks using channel configurations of [64, 256, 384, 512]. The model initialize with random weights (pretrained=False) rather then using pretrained ensuring that the model learns features specially relevant to the combined datasets.

\subsubsection{Training Setup}
The model is trained on the combination of Market-1501, MSMT17 and CUHK03 train split, resulting in 2,117 unique identities. The person ID mapping strategy described in Section \ref{imp_dataloading} ensures that identities remain unique across the combined dataset and preventing ID conflicts between datasets, while ensuring a continues range of id, optimal for training using softmax.

\subsubsection{Hyperparameters}
The complete set of training hyper parameters is presented in Table \ref{tab:osnettrainingparameters}. The model is trained by using \ac{SGD} with momentum of 0.9 and weight decay of 0.0005  for regularization. The initial learning rate is set to 0,001. The loss function used is CrossEntropyLoss with a label smoothing of 0.1. Label smoothing is a regularization technique that prevents the model for becoming overconfident in its predictions by softening the target labels. Instead of using one-hot encoded labels. This is particularly beneficial for person ReID, as it addresses the challenge of limited training samples per identity. By preventing extreme confidence on training identities, label smoothing encourages the model to learn more robust features that generalize better to unseen test identities, a critical requirement for the open-set nature of person ReID. Unlike many person ReID approaches, no triplet loss is incorporated in this implementation.
A \texttt{MultiStepLR} scheduler controls the learning rate throughout training, reducing it by a factor of 0.1 at epoch 60, 120, and 180. The results of the learning rate schedule:

\begin{itemize}
    \item Epochs 0-100: LR =0.001
    \item Epochs 100 - 174 LR = 0.0001
    \item Epochs 175 - 224 LR = 0.00001
    \item Epochs 225 - 350 LR = 0.000001

\end{itemize}
Training runs for a total of 350 epochs without early stopping, though the best 


\subsubsection{Validation Strategy}
The models performance is monitored through periodic validation:
\begin{itemize}
    \item For epochs 1 - 200: validation is performed every 10 epoch
    \item For epochs 201 - 350: validation frequency increases to every 5 epoch
\end{itemize}

This adaptive validation strategy allows for more frequent monitoring as the training and the model approaches convergence.

%Info om LR, Få mere info i næste afsnit.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}\hline
         \cellcolor[HTML]{D8E9F7} \textbf{Training Hyperparameter}&  \cellcolor[HTML]{D8E9F7} \textbf{Value}\\\hline
         No. Images &  2117\\\hline
         No. epochs &  350\\\hline
         Batch Size &  64 \\\hline
         Optimizer &  SGD \\\hline
         Momentum&0.9\\\hline
         Weight Decay&0.0005\\\hline
         Learning Rate Scheduler &\texttt{MultiStepLR}\\\hline
         Learning Rate & 0.001\\\hline
         LR Milestones&  [100, 175, 225]\\\hline
         LR Gamma&  0.1\\\hline
         Optimizer &  SGD \\\hline
         Loss Function&  CrossEntropyLoss\\\hline
         
 Label Smoothing&0.1\\ \hline
    \end{tabular}
    \caption{The training hyperparameters and their value for training the OSNet.}
\label{tab:osnettrainingparameters}
\end{table}

\subsubsection{Batch Size Considerations}


\section{Implementation of EDSR}
\label{sec:edsr_implementation}

The \ac{EDSR} model was selected for the \ac{SR} module based on its low distortion characteristics, as discussed in Section \ref{sec:sr_methods}. This section describes the implementation details, including model architecture, data loading, augmentation strategies, and training procedures.

\subsection{Model Architecture}
The baseline \ac{EDSR} configuration was used, which provides a balance between computational efficiency and reconstruction quality. The architecture consists of an initial convolutional layer that expands the input from 3 RGB channels to the feature space, followed by a stack of residual blocks, and finally an upsampling module that produces the super-resolved output. Table \ref{tab:edsr-architecture} summarizes the architectural parameters.

\begin{table}[H]
    \centering
    \caption{\acs{EDSR} baseline architecture configuration.}
    \label{tab:edsr-architecture}
    \begin{tabular}{|c|c|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Parameter} & \cellcolor[HTML]{D8E9F7}\textbf{Value} \\\hline
        Number of residual blocks (\texttt{n\_blocks}) & 16 \\\hline
        Number of feature maps (\texttt{n\_feats}) & 64 \\\hline
        Upscaling factor (\texttt{scale}) & 2 \\\hline
        Input channels (\texttt{in\_ch}) & 3 \\\hline
        Output channels (\texttt{out\_ch}) & 3 \\\hline
        Residual scaling factor (\texttt{res\_scale}) & 1.0 \\\hline
    \end{tabular}
\end{table}

\noindent Each residual block consists of two 3×3 convolutional layers with ReLU activation between them. The residual scaling factor multiplies the output of each residual block before adding it to the skip connection, which helps stabilize training in deeper networks. For the baseline configuration with 64 feature maps, a residual scaling factor of 1.0 was used, as scaling is primarily needed for wider networks with 256 feature maps.

The upsampling module uses a sub-pixel convolution approach (PixelShuffle), where a convolutional layer first expands the feature maps by a factor of $\text{scale}^2$, and the PixelShuffle operation then rearranges these features into the spatially upscaled output. This approach performs upsampling in a learnable manner rather than using fixed interpolation methods.

\subsection{Dataset and Data Loading}
Two dataset classes were implemented to handle different data formats. The \texttt{SRDataset} class loads paired \ac{LR} and \ac{HR} images from separate directories, matching files by sorted order. This class was used for the RELLISUR dataset, where cropped person images were prepared at 64×128 pixels (\ac{LR}) and 128×256 pixels (\ac{HR}).

The \texttt{DIV2KDataset} class handles the \ac{DIV2K} dataset format, where \ac{LR} images follow the naming convention \texttt{0001x2.png} corresponding to \ac{HR} images named \texttt{0001.png}. This class automatically matches \ac{LR} and \ac{HR} pairs based on filename parsing.

Both dataset classes support synchronized augmentation through a \texttt{use\_sync\_aug} parameter. When enabled, the same random seed is used for both \ac{LR} and \ac{HR} images, ensuring that geometric transformations such as horizontal flipping are applied consistently to both images in a pair.

\subsection{Data Augmentation}
A set of data augmentation transforms was implemented to improve model robustness during training. The augmentation pipeline for \ac{SR} training included the following transforms:

\begin{table}[H]
    \centering
    \caption{Data augmentation transforms for \acs{SR} training.}
    \label{tab:sr-augmentation}
    \begin{tabular}{|l|c|l|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Transform} & \cellcolor[HTML]{D8E9F7}\textbf{Probability} & \cellcolor[HTML]{D8E9F7}\textbf{Parameters} \\\hline
        Horizontal Flip & 0.5 & - \\\hline
        Brightness Change & 0.6 & Factor range: [0.3, 1.0] \\\hline
        Gaussian Noise & 0.5 & Mean: 0.0, Std: 0.1 \\\hline
        Gaussian Blur & 0.5 & Kernel size: 7, Sigma: [0.1, 2.0] \\\hline
    \end{tabular}
\end{table}

\noindent Horizontal flip and brightness change were applied with synchronized random states between \ac{LR} and \ac{HR} images to maintain correspondence. Gaussian noise and Gaussian blur were applied independently to simulate realistic degradations. All images were normalized to the range [-1, 1] using mean and standard deviation of 0.5 per channel.

For validation, no augmentation was applied. Images were converted to tensors and normalized using the same normalization parameters as training.

\subsection{Training Configuration}
Two training strategies were employed: training from scratch on RELLISUR, followed by finetuning the best model on \ac{DIV2K}.

\subsubsection{Training from Scratch on RELLISUR}
The model was first trained from scratch on cropped RELLISUR data with random weight initialization. The training configuration used an Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 10^{-8}$. The learning rate was set to $1 \times 10^{-4}$ and kept constant throughout training. The best checkpoint was selected based on the highest validation \ac{PSNR}.

\subsubsection{Finetuning on DIV2K}
The best checkpoint from the scratch training (epoch 113) was used as the starting point for finetuning on the \ac{DIV2K} dataset. This finetuning step aimed to improve generalization by exposing the model to a more diverse set of high-quality images with varied content. For finetuning, the learning rate was reduced to $5 \times 10^{-6}$ to preserve the learned RELLISUR features while allowing adaptation to the broader \ac{DIV2K} distribution. The same Adam optimizer configuration was used.

\subsubsection{Loss Function}
The L1 loss (mean absolute error) was used as the training objective:
\begin{equation}
    \mathcal{L}_{L1} = \frac{1}{N} \sum_{i=1}^{N} |SR_i - HR_i|
\end{equation}
where $SR_i$ is the super-resolved output and $HR_i$ is the ground truth \ac{HR} image. L1 loss was chosen over L2 loss (mean squared error) as it produces sharper results and is less sensitive to outliers.

\subsubsection{Model Selection}
The best model checkpoint was selected based on the highest validation \ac{PSNR} rather than the lowest validation loss. This selection criterion was chosen to prioritize reconstruction quality as measured by \ac{PSNR}, which directly reflects the pixel-level accuracy of the super-resolved images.

\subsection{Training Hyperparameters}
Table \ref{tab:edsr-hyperparameters} summarizes the complete set of training hyperparameters for both training stages.

\begin{table}[H]
    \centering
    \caption{Training hyperparameters for \acs{EDSR}.}
    \label{tab:edsr-hyperparameters}
    \begin{tabular}{|l|c|c|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Hyperparameter} & \cellcolor[HTML]{D8E9F7}\textbf{Scratch (RELLISUR)} & \cellcolor[HTML]{D8E9F7}\textbf{Finetuned (DIV2K)} \\\hline
        Training Dataset & RELLISUR Cropped & DIV2K \\\hline
        Initial Weights & Random & Best RELLISUR checkpoint \\\hline
        Number of Epochs & 200 & 100 \\\hline
        Batch Size & 16 & 16 \\\hline
        Optimizer & Adam & Adam \\\hline
        Learning Rate & $1 \times 10^{-4}$ & $5 \times 10^{-6}$ \\\hline
        $\beta_1$, $\beta_2$ & 0.9, 0.999 & 0.9, 0.999 \\\hline
        Loss Function & L1 Loss & L1 Loss \\\hline
        LR Input Size & 64 × 128 & 64 × 128 \\\hline
        HR Output Size & 128 × 256 & 128 × 256 \\\hline
        Data Augmentation & Synchronized & Synchronized \\\hline
        Model Selection & Best Val PSNR & Best Val PSNR \\\hline
    \end{tabular}
\end{table}

\subsection{Validation Strategy}
Validation was performed at the end of each epoch using the RELLISUR validation split. The validation pipeline computed both validation loss (L1) and validation \ac{PSNR} to monitor training progress. \ac{PSNR} was calculated by first denormalizing the images from [-1, 1] to [0, 1], then computing the mean squared error between super-resolved and ground truth images:
\begin{equation}
    \text{PSNR} = 20 \cdot \log_{10}\left(\frac{1}{\sqrt{\text{MSE}}}\right)
\end{equation}
Model checkpoints were saved every 10 epochs, and the best model based on validation \ac{PSNR} was saved separately for final evaluation.


\section{Implementation of Sequential ReID and SR}
To fully asses the impact of implementing a \ac{SR} system with \ac{ReID} a script was created to evaluate a sequential combination of the two models, where the data is first passed through the trained \ac{SR} model, whose output is then fed into the trained \ac{ReID} model. The efficiency of this setup is expressed with error metrics mAP and Rank-1, and can then be compared with other evaluation instances, such as the same \ac{ReID} model without the \ac{SR}. 
\\\\
This sequential evaluation script is made to test both \ac{ReID} alone and also in combination with \ac{SR}. It has been specifically made to fit only the OSNet and EDSR structure, as those are the main models used in this project. The dataloader discussed in \ref{imp_dataloading} has also been used in conjunction with this script, but mostly with a distinct emphasis on the evaluation part, such as query and gallery for the relevant \ac{ReID} datasets. 
% Noget om forskellige sr scales prøvet. Batch sizes, num workers?
% Baseline models tested (off the shelf måske?)
% Den wrap ved "class IntPidCamDataset" kan også forklares hvis det er.
% Noget om feature extraction måske?
% Noget om alle de forskellige args?




\section{Implementation of Joint ReID and SR}
Since the \ac{SR} module is trained to upscale images, it is not specialized for the task of person \ac{ReID}. By combining the two modules end-to-end, the \ac{ReID} loss can flow back into the \ac{SR} module, forcing it to learn specific upscaling and cleaning operations which improve the final \ac{ReID} task. 
\\\\
To combine the two modules end to end, each model was downloaded from it’s respective github repository. The models were then wrapped in a function which builds the model with the configuration befitting it’s size. OSNet configuration can be seen in Table \ref{tab:osnet-config}, EDSR configuration can be seen in Table \ref{tab:edsr-config}.

\noindent To allow for losses to backpropagate through both modules, a new class was created with a combined forward pass. The forward pass takes an image as input, forwards it through the \ac{SR} module first and then through the \ac{ReID} module, with the option of also returning the \ac{SR} output to increase operation transparency. The class also allows for each module to be initated with frozen weights, such that the possibility of the \ac{SR} module becoming a \ac{ReID} feature extractor is minimized. The frozen module can then be unfrozen with an unfreeze function.