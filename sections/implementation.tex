\chapter{Implementation} \label{cha: implementation}
This Chapter presents the implementation of the dataloader and training scripts, as well as the chosen \acs{SR} and \acs{ReID} architecture, and the combination of these.
SR and ReID together with the overall implementation choices.

\section{Dataloaders}
\label{imp_dataloading}
Two data loaders were built using the \texttt{Dataset} and \texttt{DataLoader} classes from \texttt{torch.utils.data}. They are designed to generate batches of data samples for training, validation and test. Special for the \ac{ReID} data loader is that validation and test is divided into query and gallery splits. For the \ac{ReID} datasets, three datasets were used for training MSMT17 \cite{wei2018person}, Market-1501 \cite{Zheng2015ICCV}  and CUHK03 \cite{KaggleCUHK03}



The data loader combines these datasets, enabling joint training across all of them. To prevent any person-id conflict, in cases where the same person-id appears in multiple datasets, and to ensure continues ID-values, a \texttt{JSON} file with mappings was created. This maps the old ID, with a dataset specific prefix in front, to a new continues value. It ensures that all person-ID's remain unique across datasets, while preserving a person-ID for each person and introducing continues values on which softmax loss can be used. In the process of creating the \texttt{JSON} file, every 10th new person-ID is pulled into the validation set. In the validation split, one sample per person-ID is added to the query set. The remaining samples are categorized for the gallery set: Any sample that does not match both the person-ID and camera-ID of the query sample is added to the gallery.
\\\\
The data loader for the SR datasets simply splits the images into input and target images, and creates batches. The images from DIV2K \cite{Agustsson2017NTIRE} and RELLISUR \cite{aakerberg2021rellisur} dataset were kept separate.

\section{Dataaugmentation} \label{sec: dataaugmentation}
A classic way to improve performance and generalization is through augmentations. The goal is to improve model performance in terms of generalization across different image scales and resolution, for robustness in real life scenarios. The different types of augmentation and their purpose is included in Table \ref{tab:augmentationtypes}.
\begin{table}[H]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}p{0.18\linewidth}|>{\centering\arraybackslash}p{0.78\linewidth}|}\hline
         \cellcolor[HTML]{D8E9F7} \textbf{Augmentation}&  \cellcolor[HTML]{D8E9F7} \textbf{Purpose}\\\hline
         Horizontal flipping&  Increase dataset size.\\\hline
 Gaussian blur&Improve robustness to low-quality or blurred images.\\\hline
 Gaussian noise&Enhance generalization to imperfect or noisy images.\\\hline
 Brightness change&Increase resilience to varying lighting conditions.\\\hline
 Resizing&Introduces scale variation by randomly reducing the image size with a scale between 0.5 and 1 and then black padding back to the target dimensions. Resizing is performed with bilinear interpolation.\\\hline
 Padding& Secure that tensor sizes is the same in order to be stacked.\\\hline
 JPEG compression& Improve robustness to compression noise, simulating compression from surveillance footage.\\ \hline
    \end{tabular}
    \caption{The different augmentations implemented.}
    \label{tab:augmentationtypes}
\end{table}
\noindent These data augmentations were used in three different transformation pipelines: One with light augmentations, one with strong augmentations, and one for evaluations, see Table \ref{tab:three_subtables}
\begin{table}[H]
\centering
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Light Training Augmentation}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Normalization
    \item Horizontal flipping
    \item Padding, if too small
    \item Resizing, if too big
    \vspace{6.1em}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentations used for baseline version of OSNet.}
\end{subtable}
\hfill
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Strong Training Augmentation}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Gaussian noise
    \item Gaussian Blur
    \item Brightness change
    \item Resizing
    \item JPEG compression
    \item Normalization
    \item Horizontal flipping
    \vspace{1em}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentation used for EDSR and augmentation version of OSNet.}
\end{subtable}
\hfill
\begin{subtable}[T]{0.32\linewidth}
\centering
\begin{tabular}{|>{\centering\arraybackslash}p{0.9\linewidth}|}\hline
\cellcolor[HTML]{D8E9F7} \textbf{Evaluation Preprocessing}\\\hline
\begin{itemize}[leftmargin=*, topsep=0pt, itemsep=0pt]
    \item Normalization
    \vspace{11em}
    \vspace{1mm}
\end{itemize}
\\ \hline\end{tabular}
\caption{Augmentation used for validation and test sets.}
\end{subtable}

\caption{The three different augmentations pipelines.}
\label{tab:three_subtables}
\end{table}
\noindent The three augmentations pipelines will be used in Chapter \ref{cha: experiments}.

\section{Implementation of OSNet}\label{sec:implementationosnet}
%Note: vi bruger x1 modellen, som er den største. 
The architecture of the implemented person \acs{ReID} model is identical to what described in Section \ref{lab:methodreid}. The \acf{OSNet} implementation is taken from the \texttt{Torchreid} repository, which comes with a pretrained \acs{OSNet} model. However, to ensure full control over the training process and to train from scratch, a custom training script was made. 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}\hline
        \cellcolor[HTML]{D8E9F7} \textbf{Parameter} & \cellcolor[HTML]{D8E9F7} \textbf{Value} \\\hline
        No. Classes (\texttt{num\_classes}) & \texttt{num\_classes} \\\hline
        Block types (\texttt{blocks})             & [\texttt{OSBlock}, \texttt{OSBlock}, \texttt{OSBlock}] \\\hline
        Layers per stage (\texttt{layers})        & [2, 2, 2] \\\hline
        Channels per stage (\texttt{channels})    & [64, 256, 384, 512] \\\hline
        Loss function (\texttt{loss})             & \texttt{loss parameter ('softmax' or 'triplet')} \\\hline
    \end{tabular}
        \caption{OSNet\_x1\_0 Configuration.}
    \label{tab:osnet-config}
\end{table}

\subsubsection{Model Configuration}
The \acs{OSNet} x1 variant is used, which is the largest base \acs{OSNet} model with 2.2 million parameters. As shown in Table \ref{tab:osnet-config} the model consists of three stages of OSBlocks using channel configurations of [64, 256, 384, 512]. The model was initialized with random weights (pretrained=False) rather than using pretrained, ensuring that the model learns features specially relevant to the combined datasets.

\subsubsection{Training Setup}
The model was trained on the combined Market-1501, MSMT17 and CUHK03 train split, resulting in 2,117 unique identities. The person ID mapping strategy described in Section \ref{imp_dataloading} ensures that identities remain unique across the combined dataset and prevents ID conflicts between datasets, while ensuring a continues range of ID's, which is optimal for training using softmax loss.

\subsubsection{Hyperparameters}
The complete set of training hyper parameters is presented in Table \ref{tab:osnettrainingparameters}. The model was trained using \ac{SGD} with momentum set to 0.9 and weight decay set to 0.0005 for regularization. The initial learning rate was set to 0,001. The loss function used is CrossEntropyLoss with a label smoothing of 0.1. Label smoothing is a regularization technique that prevents the model from becoming overconfident in its predictions by softening the target labels. Instead of using one-hot encoded labels. This is particularly beneficial for person ReID, as it addresses the challenge of limited training samples per identity. By preventing extreme confidence on training identities, label smoothing encourages the model to learn more robust features that generalize better to unseen test identities, a critical requirement for the open-set nature of person ReID.
A \texttt{MultiStepLR} scheduler controls the learning rate throughout training, reducing it by a factor of 0.1 at epoch 100, 175, and 225. The results of the learning rate schedule:

\begin{itemize}
    \item Epochs 0-100: LR = 0.001
    \item Epochs 100 - 174 LR = 0.0001
    \item Epochs 175 - 224 LR = 0.00001
    \item Epochs 225 - 350 LR = 0.000001

\end{itemize}
Training ran for a total of 350 epochs without early stopping, though checkpoints were saved every 10 epochs, as well as the best model based on an average in performance metrics.


\subsubsection{Validation Strategy}
The model's performance was monitored through periodic validation:
\begin{itemize}
    \item For epochs 1 - 200: validation is performed every 10 epoch
    \item For epochs 201 - 350: validation frequency increases to every 5 epoch
\end{itemize}

\noindent This adaptive validation strategy allows for more frequent monitoring as learning rate drops, and the training approaches convergence.

%Info om LR, Få mere info i næste afsnit.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}\hline
         \cellcolor[HTML]{D8E9F7} \textbf{Training Hyperparameter}&  \cellcolor[HTML]{D8E9F7} \textbf{Value}\\\hline
         No. Images &  2117\\\hline
         No. epochs &  350\\\hline
         Batch Size &  64 \\\hline
         Optimizer &  SGD \\\hline
         Momentum&0.9\\\hline
         Weight Decay&0.0005\\\hline
         Learning Rate Scheduler &\texttt{MultiStepLR}\\\hline
         Learning Rate & 0.001\\\hline
         LR Milestones&  [100, 175, 225]\\\hline
         LR Gamma&  0.1\\\hline
         Optimizer &  SGD \\\hline
         Loss Function&  CrossEntropyLoss\\\hline
         
 Label Smoothing&0.1\\ \hline
    \end{tabular}
    \caption{The training hyperparameters and their value for training OSNet.}
\label{tab:osnettrainingparameters}
\end{table}

\subsubsection{Batch Size Considerations}


\section{Implementation of EDSR}
\label{sec:edsr_implementation}

The \ac{EDSR} model was chosen as the \ac{SR} module for this project because it has low distortion, as discussed in Section \ref{sec:SuperResolution}, which is important for the \ac{ReID} task where we need to keep useful features. This section presents the implementation details, including model architecture, data loading, augmentation strategies, and training procedures.

\subsubsection{Model Architecture}
The baseline \ac{EDSR} configuration was used, which balances computational efficiency and reconstruction quality. Table \ref{tab:edsr-architecture} summarizes the architectural parameters.

\begin{table}[H]
    \centering
    \caption{\acs{EDSR} baseline architecture configuration.}
    \label{tab:edsr-architecture}
    \begin{tabular}{|c|c|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Parameter} & \cellcolor[HTML]{D8E9F7}\textbf{Value} \\\hline
        Number of residual blocks (\texttt{n\_blocks}) & 16 \\\hline
        Number of feature maps (\texttt{n\_feats}) & 64 \\\hline
        Upscaling factor (\texttt{scale}) & 2 \\\hline
        Input channels (\texttt{in\_ch}) & 3 \\\hline
        Output channels (\texttt{out\_ch}) & 3 \\\hline
        Residual scaling factor (\texttt{res\_scale}) & 1.0 \\\hline
    \end{tabular}
\end{table}

\noindent Each residual block consists of two 3×3 convolutional layers with ReLU activation between them. The residual scaling factor multiplies the output of each residual block before adding it to the skip connection, which helps stabilize training in deeper networks. For the baseline configuration with 64 feature maps, a residual scaling factor of 1.0 was used, as scaling is primarily needed for wider networks with 256 feature maps.
\\\\ % Paragrafen over og under her, Peter !
The upsampling module uses a sub-pixel convolution approach (\texttt{PixelShuffle}), where a convolutional layer first expands the feature maps by a factor of $\text{scale}^2$, and the \texttt{PixelShuffle} operation then rearranges these features into the spatially upscaled output. This approach performs upsampling in a learnable manner rather than using fixed interpolation methods.
\\\\
\subsubsection{Dataset and Data Loading}
Two dataset classes were implemented to work with different data formats. The \texttt{SRDataset} class loads paired \ac{LR} and \ac{HR} images from separate directories, matching files by sorted order. This class was used for the RELLISUR dataset, where cropped person images were prepared at 64×128 pixels (\ac{LR}) and 128×256 pixels (\ac{HR}).
\\\\
The \texttt{DIV2KDataset} class works with the \ac{DIV2K} dataset format, where \ac{LR} images follow the naming convention \texttt{0001x2.png} corresponding to \ac{HR} images named \texttt{0001.png}. This class matches \ac{LR} and \ac{HR} pairs based on filename parsing.
\\\\
Both dataset classes support synchronized augmentation through a \texttt{use\_sync\_aug} parameter. When enabled, the same random seed is used for both \ac{LR} and \ac{HR} images, so that geometric transformations such as horizontal flipping are applied consistently to both images in a pair.

\subsubsection{Data Augmentation}
A set of data augmentation transforms was implemented to improve model robustness during training. The augmentation pipeline for \ac{SR} training includes the following transforms:

\begin{table}[H]
    \centering
    \caption{Data augmentation transforms for \acs{SR} training.}
    \label{tab:sr-augmentation}
    \begin{tabular}{|l|c|l|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Transform} & \cellcolor[HTML]{D8E9F7}\textbf{Probability} & \cellcolor[HTML]{D8E9F7}\textbf{Parameters} \\\hline
        Horizontal Flip & 0.5 & - \\\hline
        Brightness Change & 0.6 & Factor range: [0.3, 1.0] \\\hline
        Gaussian Noise & 0.5 & Mean: 0.0, Std: 0.1 \\\hline
        Gaussian Blur & 0.5 & Kernel size: 7, Sigma: [0.1, 2.0] \\\hline
    \end{tabular}
\end{table}

\noindent Horizontal flip and brightness change were applied with synchronized random states between \ac{LR} and \ac{HR} images to keep them matching. Gaussian noise and Gaussian blur were applied independently to simulate realistic degradations. All images were normalized to the range [-1, 1] using mean and standard deviation of 0.5 per channel.
\\\\
For validation, no augmentation was applied. Images were converted to tensors and normalized using the same normalization parameters as training.

\subsubsection{Training Configuration}
Two training strategies were used: training from scratch on RELLISUR, followed by finetuning the best model on \ac{DIV2K}. The model was first trained from scratch on cropped RELLISUR data with random weight initialization using an Adam optimizer. The learning rate was set to $1 \times 10^{-4}$ and kept constant throughout training. The best checkpoint was selected based on the highest validation \ac{PSNR}.

The best checkpoint from the scratch training (epoch 113) was then used as the starting point for finetuning on the \ac{DIV2K} dataset. This finetuning step was done to improve generalization by giving the model more diverse image content. For finetuning, the learning rate was reduced to $5 \times 10^{-6}$ to keep what it learned on RELLISUR while still learning from the broader \ac{DIV2K} dataset.
\\\\
The L1 loss (mean absolute error) was used as the training objective. L1 loss was chosen over L2 loss (mean squared error) as it produces sharper results and is less sensitive to outliers. The best model checkpoint was selected based on the highest validation \ac{PSNR} rather than the lowest validation loss. This was chosen because \ac{PSNR} shows how accurate the super-resolved images are at the pixel level. A \ac{PSNR} above 30 dB on average shows that the model produces super-resolved images with good enough quality to use in practice, which means the model is useful for the \ac{ReID} application.

\subsubsection{Hyperparameters}
The complete set of training hyperparameters for both training stages is presented in Table \ref{tab:edsr-hyperparameters}. The number of epochs used (200 for scratch training, 100 for finetuning) was chosen based on the training curves showing convergence, as well as time constraints. Looking at the training curves in Figure \ref{fig:edsr_comparison}, the models showed clear convergence patterns, and while additional training epochs might have given small improvements (potentially 1-2\% better performance), the current training duration was good enough to show how the approach works and how the models learn from the data. The learning rate of $1 \times 10^{-4}$ was selected based on what worked well with our training data, which differs from the original EDSR paper's training setup. The original EDSR was trained on more similar datasets with similar image characteristics, which allowed for higher learning rates. Our combined and cropped datasets contain more diverse content and preprocessing steps, requiring a lower learning rate to make sure training stays stable and does not diverge.


\begin{table}[H]
    \centering
    \caption{Training hyperparameters for \acs{EDSR}.}
    \label{tab:edsr-hyperparameters}
    \begin{tabular}{|l|c|c|}\hline
        \cellcolor[HTML]{D8E9F7}\textbf{Hyperparameter} & \cellcolor[HTML]{D8E9F7}\textbf{Scratch (RELLISUR)} & \cellcolor[HTML]{D8E9F7}\textbf{Finetuned (DIV2K)} \\\hline
        Training Dataset & RELLISUR Cropped & DIV2K \\\hline
        Initial Weights & Random & Best RELLISUR checkpoint \\\hline
        Number of Epochs & 200 & 100 \\\hline
        Batch Size & 16 & 16 \\\hline
        Optimizer & Adam & Adam \\\hline
        Learning Rate & $1 \times 10^{-4}$ & $5 \times 10^{-6}$ \\\hline
        $\beta_1$, $\beta_2$ & 0.9, 0.999 & 0.9, 0.999 \\\hline
        Loss Function & L1 Loss & L1 Loss \\\hline
        LR Input Size & 64 × 128 & 64 × 128 \\\hline
        HR Output Size & 128 × 256 & 128 × 256 \\\hline
        Data Augmentation & Synchronized & Synchronized \\\hline
        Model Selection & Best Val PSNR & Best Val PSNR \\\hline
    \end{tabular}
\end{table}

\subsubsection{Validation Strategy}
Validation was performed at the end of each epoch using the RELLISUR validation split. The validation pipeline computed both validation loss (L1) and validation \ac{PSNR} to monitor training progress. Model checkpoints were saved every 10 epochs, and the best model based on validation \ac{PSNR} was saved separately for final evaluation.



\section{Implementation of Sequential ReID and SR}
To fully asses the impact of implementing a \ac{SR} system with \ac{ReID}, a script was created to evaluate a sequential combination of the two models, where the data is first passed through the trained \ac{SR} model, whose output is then fed into the trained \ac{ReID} model. The efficiency of this setup is expressed with error metrics mAP and Rank-1, and can then be compared with other evaluation instances, such as the same \ac{ReID} model without the \ac{SR} module. 
\\\\
This sequential evaluation script is made to test both \ac{ReID} alone and also in combination with \ac{SR}. It was specifically made to fit only the OSNet and EDSR structure, as those are the main models used in this project. The dataloader discussed in \ref{imp_dataloading} was also used in conjunction with this script, but mostly with a distinct emphasis on the evaluation part, such as query and gallery for the relevant \ac{ReID} datasets. 
% Noget om forskellige sr scales prøvet. Batch sizes, num workers?
% Baseline models tested (off the shelf måske?)
% Den wrap ved "class IntPidCamDataset" kan også forklares hvis det er.
% Noget om feature extraction måske?
% Noget om alle de forskellige args?




\section{Implementation of Joint ReID and SR}
Since the \ac{SR} module was trained to upscale images, it is not specialized for the task of person \ac{ReID}. By combining the two modules end-to-end, the \ac{ReID} loss can flow back into the \ac{SR} module, forcing it to learn specific upscaling and cleaning operations which improve the final \ac{ReID} task. 
\\\\
To combine the two modules end to end, each model was downloaded from it’s respective Github repository. The models were then wrapped in a function which builds the model with the configuration befitting it’s size. OSNet configuration can be seen in Table \ref{tab:osnet-config}, EDSR configuration can be seen in Table \ref{tab:edsr-config}.
\\\\
To allow for losses to back-propagate through both modules, a new class was created with a combined forward pass. The forward pass takes an image as input, forwards it through the \ac{SR} module first and then through the \ac{ReID} module, with the option of also returning the \ac{SR} output to increase operation transparency. The class also allows for each module to be initiated with frozen weights, such that the possibility of the \ac{SR} module becoming a \ac{ReID} feature extractor is minimized. The frozen module can then be unfrozen with an unfreeze function.
\\\\
To benefit from the end to end integration of the two models, training was continued with pretrained weights from baseline \ac{OSNet} and finetuned \ac{EDSR}. \ac{OSNet} weights were frozen for the first 30 epochs, such that \ac{EDSR} could adapt to the new data being \ac{LR} images of people, instead of the more diverse nature of DIV2K and RELLISUR. While only a classification loss back-propagated, it should still help specialize the model for the task, as can be seen in Section XXX. The learning rate for \ac{EDSR} was chosen, due to it being a middle point between the training and finetuning learning rate used during the standalone training. The learning rate for \ac{OSNet} was set to \texttt{1e-3}, such that it would drop to \texttt{1e-4} after the the first milestone, which coincides with \ac{OSNet} being unfrozen. This was chosen as the model had already learned the task of \ac{ReID}, and in this joint training should focus on learning to extract features from \ac{SR} images. A second milestone was set at epoch 60 for final finetuning with the lowest learning rate. Training concluded at epoch 95, at which point the model had learned enough for a fair comparison with the other models, even if more performance could have been squeezed out. 
\subsection*{Training Configuration for Joint ReID and SR Model}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}\hline
         \cellcolor[HTML]{D8E9F7} \textbf{Training Hyperparameter}&  \cellcolor[HTML]{D8E9F7} \textbf{Value}\\\hline
        No. classes &  2117\\\hline
        No. epochs &  95\\\hline
        No. epochs with frozen OSNet& 30\\\hline
        Batch Size &  64 \\\hline
        Optimizer &  SGD \\\hline
        Momentum&0.9\\\hline
        Weight Decay&0.0005\\\hline
        Learning Rate Scheduler &\texttt{MultiStepLR}\\\hline
        EDSR Learning Rate & 1e-5\\\hline
        OSNet Learning Rate & 1e-3\\\hline
        LR Milestones&  [30, 60]\\\hline
        LR Gamma&  0.1\\\hline
        Loss Function&  CrossEntropyLoss\\\hline 
        Label Smoothing&0.1\\ \hline
    \end{tabular}
    \caption{The training hyperparameters and their value for training Joint model.}
\label{tab:osnettrainingparameters}
\end{table}

\section{Deployment}
\label{deployment}
As the \ac{ReID} system was made with the intention to be utilized in a real-world setting, a script was created that allows it to be deployed on a third-party system with API functionality, as to facilitate its use by other external systems and users if needed.
\\\\
To ensure a more streamlined implementation of the system in future uses, the deployment was made with a Docker containerization setup. This ensures that relevant libraries, dependencies, and any other version-specific parameters needed to run the script, are present and being used. Furthermore, a requirements text file with used libraries and corresponding versions has also been included to allow reproducibility of the container or another virtual environment if needed.
\\\\
The deployment script utilizes the \texttt{FastAPI} library as its core framework for web related communication. This is specifically chosen because of its ease of use, yet powerful foundation, and tools. As deployment was not the primary focus point of this project, only a simple \texttt{GET} and \texttt{POST} request method were integrated in the script, which runs through hard-coded image directories located in the server. with little to no front end in mind. If implementation on a real system was the end-goal of this project, more work should be done to achieve an acceptable industry standard.
\\\\
The backend pipeline of the deployment script works by first processing the gallery and query image(s) from a \texttt{.JPG} filetype to a base64 encoding and a tensor. The tensor contains the image with preprocessing applied, such as normalization and image resizing, which is done to align with the \ac{ReID} models architecture for optimal compatibility. Alternatively, the base64 encryption is a copy of the non preprocessed image that is used to make file handling between the client and server more efficient. Subsequently, the tensors for query and gallery are computed by the \ac{ReID} model, which returns the best matches back to the client. A simple HTML setup has been created to integrate some visualization for the front end, by displaying the returned matches with a ranking order and the original query image for comparison as seen in Figure \ref{fig:deployment_example}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{deployment_example.png}
    \caption{Screenshot of a localhost with the deployment script implemented. This image is after the script has computed the top ranking matches in the gallery as seen in the figure.}
    \label{fig:deployment_example}
\end{figure}






%==============================flyttet fra results

\subsection{EDSR Training}
Two training strategies were tested for the \ac{EDSR} model. The first approach trained the model from scratch on RELLISUR data. The second approach took the best checkpoint from this model and finetuned it on \ac{DIV2K} to improve generalization. Both models used the baseline configuration described in Section \ref{sec:edsr_implementation}.
\\\\
The model trained from scratch on RELLISUR showed a normal learning progression. Training loss started at approximately 0.12 and decreased quickly to around 0.095 within the first 25 epochs. The loss continued to decrease slowly and stabilized around 0.089 after epoch 150. Validation \ac{PSNR} started at 23.7 dB and improved to approximately 25.3 dB within the first 25 epochs. The highest validation \ac{PSNR} of 25.72 dB was achieved at epoch 82, and the best model checkpoint was saved at epoch 113 based on validation \ac{PSNR}. After this point, validation \ac{PSNR} fluctuated between 25.3 and 25.7 dB without further improvement, showing that the model had converged. Based on the training curves shown in Figure \ref{fig:edsr_comparison}, the model might have achieved slightly better results with additional training epochs, but the current training duration was good enough to show how the approach works and how the model learns from the data.
\\\\
The best checkpoint from the scratch model (epoch 113) was then finetuned on \ac{DIV2K} to give the model more diverse image content. This finetuning showed a different training pattern. Training loss started at approximately 0.078, which is much lower than the initial scratch training, and decreased only slightly over the course of training. This low initial loss showed that the RELLISUR-trained weights already learned useful \ac{SR} features. The highest validation \ac{PSNR} of 25.49 dB was achieved at epoch 1, after which validation \ac{PSNR} gradually declined to approximately 25.18 dB. This shows that training on \ac{DIV2K} moved the model away from the RELLISUR validation distribution. A reduced learning rate of $5 \times 10^{-6}$ was used to prevent the model from forgetting what it learned on RELLISUR.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/images/cropped_models_comparison.png}
    \caption{Comparison of training curves for \ac{EDSR} trained from scratch on RELLISUR (blue) and the same model finetuned on \ac{DIV2K} (green). Top left shows training loss, top right shows validation loss, bottom left shows validation \ac{PSNR}, and bottom right shows the learning rate schedule.}
    \label{fig:edsr_comparison}
\end{figure}

\noindent Figure \ref{fig:edsr_comparison} compares the training curves of both models. The scratch model achieved higher validation \ac{PSNR} but the finetuned model had lower training loss throughout. This difference can be explained by the different training distributions. The scratch model focused heavily on the cropped RELLISUR validation images, while the finetuned model learned to work better on new data from the diverse \ac{DIV2K} dataset.
\\\\
When tested on the RELLISUR NLHR test set (85 native low-resolution images), the finetuned model achieved an average \ac{PSNR} of 33.30 dB, while the scratch model achieved 31.77 dB. This 1.53 dB improvement shows that the finetuning strategy worked as intended. Interestingly, while the scratch model had higher validation \ac{PSNR} during training (25.72 dB vs 25.49 dB), it performed worse on the test set. This shows that validation performance on cropped images does not always show how well the model will work on full native low-resolution images. Training on diverse \ac{DIV2K} content helped the finetuned model work better on the test set. Both models reached a \ac{PSNR} above 30 dB on average, which shows that the models are useful for the \ac{SR} task and can produce super-resolved images with good enough quality for later use in applications such as \ac{ReID}. Individual test images showed \ac{PSNR} values ranging from approximately 20 dB to 42 dB. This variation depends on image content, lighting conditions, and the original capture resolution.