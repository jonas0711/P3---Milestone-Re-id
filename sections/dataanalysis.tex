\chapter{Data Analysis} \label{cha:data}
This chapter presents the \ac{ReID} and \ac{SR} datasets used in this project. The selected \ac{ReID} datasets represent realistic surveillance conditions and are established datasets within person \ac{ReID} research, ensuring comparability across studies \cite{ZhouSurvey2021}. Furthermore, two \ac{SR} dataset have been selected and presented.

\section{Overview of ReID Datasets}
Datasets were chosen based on open academic access, realistic surveillance conditions, well-defined train/query/gallery splits, multiple viewpoints per identity, and resolution variation. \cite{ZhouSurvey2021}. All dataset provide images cropped to bounding boxes around the person, making it directly usable for training of \ac{ReID} models. An overview of the datasets used for training and evaluation is shown in Table \ref{tab:reid_datasets}.
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{
|>{\raggedright\arraybackslash}p{0.18\linewidth}
|>{\centering\arraybackslash}p{0.11\linewidth}
|>{\centering\arraybackslash}p{0.12\linewidth}
|>{\centering\arraybackslash}p{0.07\linewidth}
|>{\centering\arraybackslash}p{0.07\linewidth}
|>{\centering\arraybackslash}p{0.08\linewidth}
|>{\centering\arraybackslash}p{0.05\linewidth}
|>{\raggedright\arraybackslash}p{0.12\linewidth}|}
\hline
\rowcolor[HTML]{D8E9F7}
\textbf{Dataset} & \textbf{Unique IDs (train/test)} & \textbf{Images total} & \textbf{Train} & \textbf{Query} & \textbf{Gallery} & \textbf{Cam} & \textbf{Annotation} \\ \hline
Market-1501 & 751/750 & 32,668 & 12,936 & 3,368 & 19,732 & 6 & DPM \\ \hline
DukeMTMC-ReID & 702/702 & 36,411 & 16,522 & 2,228 & 17,661 & 8 & Manual \\ \hline
CUHK03 (labeled / detected) & 767/700 & 14,097 & 7,368 & 1,400 & 5,328 & 10 & Manual / DPM \\ \hline
 MSMT17& 1,041/3,060& 126,441& 32,621& 11.659& 82.161& 15&Faster R-CNN\\\hline
\end{tabular}
\caption{Overview of person re-identification datasets used in this project. Figures follow official dataset documentation.}
\label{tab:reid_datasets}
\end{table}

\subsection*{Market-1501}
\noindent Market-1501 is one of the most widely used datasets for person \ac{ReID}. It is recorded at Tsinghua University in Beijing using six surveillance cameras: five in HD and one in SD resolution. The dataset contains 32,668 cropped images of 1,501 unique individuals, each appearing in at least two camera views. Bounding boxes are generated automatically with a Deformable Part Model detector, and the resulting person crops are resized (normalized) to a fixed resolution of 128x64 pixels. In the released dataset, all images are therefore provided at this normalized resolution of 128x64 pixels. Because the detections are automatic, some bounding boxes are misaligned or partially cropped, which makes the dataset realistic for providing material under real world surveillance conditions \cite{Zheng2015ICCV}.

\subsection*{MSMT17}
\ac{MSMT17} was recorded by a network of 15 surveillance cameras deployed across indoor and outdoor environments, capturing video footage over an extended period. The dataset was designed to address real-world challenges in person \ac{ReID} including complex lighting variations, diverse viewpoint changes, and the scale requirements of large camera networks. It contains 126,441 annotated bounding boxes of 4,101 unique identities. Pedestrian bounding boxes are detected automatically using a Faster R-CNN detector, and the resulting person crops are typically resized (normalized) to a fixed input resolution of 256x128 pixels for training, so that all input images have the same size. The raw videos were captured during 12 different time slots throughout the day, spanning about 180 hours of footage in total. This temporal coverage results in lighting variations, ranging from bright daylight to low light evening conditions, which closely mirrors the operational conditions of real-world surveillance systems. The combination of indoor and outdoor scenes, long-duration recording, and diverse environmental conditions enables \ac{ReID} models trained on MSMT17 to better generalize to real-world surveillance scenarios with varying lighting, viewpoints, and scale \cite{wei2018person}.

\subsection*{CUHK03}
\noindent
CUHK03 was collected at The Chinese University of Hong Kong and was one of the first ReID datasets large enough to support deep learning. It consists of approximately 14,000 images of about 1,467 individuals captured by ten cameras, which are grouped into five disjoint camera pairs.  Each person appears in two cameras, giving two different viewpoints for every identity.  The dataset is released in two versions: a labeled set with manually annotated bounding boxes, and a detected set generated by the same \ac{DPM} detector used in Market-1501 \cite{Li2014DeepReID}.  
While CUHK03 is smaller than Market-1501 and DukeMTMC-ReID, its clear structure and the two annotation versions make it well suited for providing our model noise and viewpoint variation in the training data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/images/ReID.png}
    \caption{Enter Caption}
    \label{fig:ReIddataset}
\end{figure}

\subsection*{DukeMTMC-ReID}
\noindent
DukeMTMC-ReID is from the Duke Multi-Target Multi-Camera project and was recorded on the Duke University campus using eight 1080p cameras. The dataset contains 36,411 person images divided into 702 training and 702 test identities. 
The data capture diverse viewpoints, lighting variations, and scales from close-up pedestrians to distant, low-resolution figures which provides realistic footage from security cameras where the distance to the person can differ \cite{Gou2017DukeMTMC4ReID}. This diversity strengthens evaluation across different camera positions and environmental conditions.

\subsection*{ILIDS-VIDS}
The \ac{iLIDS-VID} dataset is captured in an airport arrival hall under a multi camera CCTV network. The dataset consists of 600 image sequences capturing 300 distinct individuals, with one pair of image sequences from two non overlapping camera views for each person. The dataset is provided in two versions, a static images version where single frames are randomly selected from each sequence, and an image sequences version for video-based ReID evaluation. For this project, the image version will be used. \ac{iLIDS-VID} is considered highly challenging due to several factors like clothing similarities among individuals, significant lighting and viewpoint variations across camera views, cluttered backgrounds with heavy occlusion and the airport environment. These characteristics make the dataset particularly valuable for evaluating ReID methods under realistic surveillance conditions, where temporal information from video sequences can help overcome the visual vagueness present in single frame matching \cite{wang2014person}.

%har to udgaver: en med hele videosekvenser og en med kun et enkelt billede fra hver sekvens.

\section{Choice of Dataset}
The three datasets complement each other. They cover multi-camera setups, varied viewpoints, and include both manual and detector-generated crops. \textit{Market-1501} contributes realistic detector noise, \textit{CUHK03} provides a clean labeled split plus a detected split that mirrors real pipelines, \textit{DukeMTMC-ReID} adds clear scale variation, where the same person appears both near and far. Together, they offer a solid basis for training and testing across camera changes, viewpoint shifts, and moderate lighting differences.
\\\\
The coverage of culture, weather, and geography is narrow. All datasets are from single university campuses (two in East Asia, one in the United States), mainly in daytime. There is little variation in season and weather (e.g., rain, snow, fog) and limited night scenes. Clothing styles and demographics reflect local contexts rather than global diversity. Automatic boxes improve realism but can include background clutter and partial crops. Manual boxes are cleaner but less like real pipelines. In short, the datasets are strong benchmarks for comparable settings. For use in other places—different cities, climates, or cultural contexts—models may need domain adaptation or a small amount of local data.

\section{Super Resolution Datasets}

\subsection*{Div2K}
\ac{DIV2K} was introduced as part of the NTIRE 2017 Challenge on Single Image Super-Resolution \cite{Agustsson_2017_CVPR_Workshops}. The dataset consists of 1,000 high-quality RGB images manually collected from the Internet, with careful attention to image quality, diversity of sources, content variety, and specially copyright considerations. Images are of 2K resolution. The dataset is split into 800 training images and 100 validation images, with an additional 100 test images. For each HR image, corresponding LR images are provided at downscaling factors of ×2, ×3, and ×4, generated using bicubic interpolation. Additionally unknown degradation track provides LR images created with hidden degradation operators to try to simulate realistic scenarios. The images were selected to achieve balanced distribution across visual content bit-per-pixel compression rates. featuring diverse content including normal scenes, textures, objects, and patterns. This diversity and high quality make \acs{DIV2K} widely used benchmarks for training and evaluating \acs{SR} algorithms \cite{Agustsson_2017_CVPR_Workshops}.

\subsection*{RELLISUR}
\noindent
RELLISUR was collected by Aalborg University focused on image resolution, not digital downsampling. Each of the 850 scenes are recorded in 3 different scales. X1 scale, X2 scale and X4 scale are made by changing the focal length. This difference in resolution originates from the optical settings and not made by bicubic manipulation. The dataset contains of a low light and a normal light part. This project uses the normal light consisting of 2,550 images split into test, train and validation. RELLISUR covers indoor and outdoor environments, and consists of different kinds of structures, materials, and distances to the objects to enhance usability for \ac{SR} in realistic environments \cite{aakerberg2021rellisur}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/images/SR.png}
    \caption{Enter Caption}
    \label{fig:placeholder2}
\end{figure}






\